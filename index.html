<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://zhang-mohole.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zhang-mohole.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-202004232043-CVPR2020-SLT" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/23/202004232043-CVPR2020-SLT/" class="article-date">
  <time datetime="2020-04-23T12:43:37.000Z" itemprop="datePublished">2020-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/23/202004232043-CVPR2020-SLT/">202004232043-CVPR2020-SLT</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Sign-Language-Transformers-Joint-End-to-end-Sign-Language-Recognition-and-Translation"><a href="#Sign-Language-Transformers-Joint-End-to-end-Sign-Language-Recognition-and-Translation" class="headerlink" title="Sign Language Transformers:Joint End-to-end Sign Language Recognition and Translation"></a>Sign Language Transformers:Joint End-to-end Sign Language Recognition and Translation</h1><p><em>note at:</em> 202004232043</p>
<p><em>labels:</em> SLT</p>
<p><strong>conf:</strong> CVPR 2020</p>
<p><strong>author:</strong> Necati Cihan Camgoz, Oscar Koller</p>
<p><strong>codes available at:</strong> <a href="https://github.com/neccam/slt" target="_blank" rel="noopener">code link</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/23/202004232043-CVPR2020-SLT/framework.png" alt="framework overview"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><blockquote>
<p>motivation</p>
</blockquote>
<p>本文的主要目的是希望通过end2end的方式，直接进行从sign language到自然语言text的手语翻译，同时利用中间态gloss标注作为中间监督，从而使得不会出现sign2gloss2text这种在gloss这里的瓶颈。</p>
<blockquote>
<p>problems</p>
</blockquote>
<ul>
<li>之前的大部分工作都是做CSLR任务，默认sign video跟text之间是一一对应的，没有复杂的语序等问题</li>
<li>之前的SLT工作利用了gloss中间态监督信息，但是其分步骤进行翻译的结构使得在得到gloss后形成了一个瓶颈，内在的失去了一些视频中的信息</li>
<li>手语本身也存在着难以理解的内在复杂性。手语是个集多种信息于一体的语言，在视频中不止有手，包括动作，头，身体，表情等等都是信息的携带点。此外，手语也是一种语言，跟spoken language一样，也是有自己的语法和词汇的，所以默认两者之间存在着简单的直接映射是不合理的。</li>
</ul>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><blockquote>
<p>手语处理流程</p>
</blockquote>
<ul>
<li>视觉信息理解<ul>
<li>这里，是个很重要的环节。在sign2text的模型中，缺少sign2gloss过程，这就相当于直接把视频翻译成自然语言，跨过了对视频的直接理解监督，这种模型的表现普遍不如利用了gloss的。而利用gloss则是相当于在中间加入了中间层监督，这样就可以更好的控制模型对输入视频的正确理解过程。</li>
</ul>
</li>
<li>视觉信息到自然语言的映射</li>
</ul>
<blockquote>
<p>method details</p>
</blockquote>
<p><img src="/2020/04/23/202004232043-CVPR2020-SLT/method.png" alt="method framework for single layer"></p>
<p>整个网络分为三个部分：</p>
<ul>
<li>SLRT(Sign Language Recognition Transformers)<ul>
<li>整个网络框架是利用了BERT的结构，整个思路也基本上是一样的，只是输入信息不同。（BERT模型需要另做学习，这里留一个链接坑位吧<a href="？？">BERT 学习笔记</a>）<a href="https://github.com/codertimo/BERT-pytorch" target="_blank" rel="noopener">pytorch implement of BERT</a></li>
<li>input：视频编码——利用CNN提取视频帧的feature。（这里是重要的改进点！！！）</li>
<li>position embedding：同样是bert模型中的东西，因为基于transformer的模型的并行性，不像RNN那样有着天然的时序信息，这里通过position embedding来对时序进行建模。</li>
<li>后面又通过self attention来进一步加强了输入视频帧各帧feature之间的语义关系（这里可能是已经能够建模前后帧之间的关系了，如果想在这里加强上下文信息关系，需要好好思考如何进行改进，这个地方更像是纯粹的NLP问题，如果有好的方式，在其他NLP领域应该也会很有用）</li>
<li>后面经过了feed forward之后又经过了FC（linear）然后softmax算概率，最后送到CTC模块来从frame level到gloss level</li>
</ul>
</li>
<li>CTC<ul>
<li>这个模块具体内容需要更进一步学习，包括看相关代码，在NLP任务中很重要</li>
<li>CTC算法的详细讲解博客：<a href="https://xiaodu.io/ctc-explained/" target="_blank" rel="noopener">CTC 算法解析 part1</a>，<a href="https://xiaodu.io/ctc-explained-part2/" target="_blank" rel="noopener">CTC 算法解析 part2</a>; 百度CTC loss工具包：<a href="https://github.com/baidu-research/warp-ctc" target="_blank" rel="noopener">warp-ctc</a>；<a href="https://github.com/SeanNaren/warp-ctc" target="_blank" rel="noopener">another pytorch implement</a></li>
</ul>
</li>
<li>SLTT(Sign Language Translation Transformers)<ul>
<li>这个相当于是decoder，跟bert模型完全一样的</li>
</ul>
</li>
</ul>
<h2 id="experiments-and-results"><a href="#experiments-and-results" class="headerlink" title="experiments and results"></a>experiments and results</h2><blockquote>
<p>results</p>
</blockquote>
<p><img src="/2020/04/23/202004232043-CVPR2020-SLT/result1.png" alt="result1"></p>
<p><img src="/2020/04/23/202004232043-CVPR2020-SLT/result2.png" alt="result2"></p>
<p><img src="/2020/04/23/202004232043-CVPR2020-SLT/result3.png" alt="result3"></p>
<h2 id="some-comments"><a href="#some-comments" class="headerlink" title="some comments"></a>some comments</h2><p>本文总体来说算是第一次正经的好好解决了SLT的问题，虽然两年前的nslt算是开山之作，但是当时的方法很直接，也很粗暴，相当于是CSLR+NMT的简单使用，并没有做的更细致。而这一篇工作就算是SLT应该有的处理方式了，通过CNN提取视频特征，通过跟视频时序逐次对应的gloss来作为视频理解的中间监督，通过CTC来解决视频维度远大于gloss词维度的问题，然后利用更新的NMT中基于BERT的网络框架来处理翻译任务。</p>
<p>算是一个很不错的baseline，直观简单，但是确实应该这么做。后面应该还有很大改进的空间，需要在视频理解这里花一些功夫。还有就是视频特征是否可以进行更稀疏的处理，减少不必要的冗余信息。</p>
<h2 id="references-must-be-checked"><a href="#references-must-be-checked" class="headerlink" title="references must be checked!"></a>references must be checked!</h2><ul>
<li>StephanieStoll,NecatiCihanCamgoz,SimonHadfield,and Richard Bowden. <em>Sign Language Production using Neural Machine Translation and Generative Adversarial Networks.</em> In Proceedings of the British Machine Vision Conference (BMVC), 2018.</li>
<li>StephanieStoll,NecatiCihanCamgoz,SimonHadfield,and Richard Bowden. <em>Text2Sign: Towards Sign Language Pro- duction using Neural Machine Translation and Generative Adversarial Networks.</em> International Journal of Computer Vision (IJCV), 2020.</li>
<li>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. <em>Attention is All You Need.</em> In Proceedings of the Advances in Neural Information Processing Systems (NIPS), 2017.</li>
</ul>
<h2 id="inspirations"><a href="#inspirations" class="headerlink" title="inspirations"></a>inspirations</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/23/202004232043-CVPR2020-SLT/" data-id="ck9cxe8b300002kfyczbv64df" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CVPR2020/" rel="tag">CVPR2020</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SLR/" rel="tag">SLR</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004221543-CVPR2020-MusicGesture" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/22/202004221543-CVPR2020-MusicGesture/" class="article-date">
  <time datetime="2020-04-22T07:42:46.000Z" itemprop="datePublished">2020-04-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/22/202004221543-CVPR2020-MusicGesture/">202004221543-CVPR2020-MusicGesture</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="A2J-Anchor-to-Joint-Regression-Network-for-3D-Articulated-Pose-Estimation-from-a-Single-Depth-Image"><a href="#A2J-Anchor-to-Joint-Regression-Network-for-3D-Articulated-Pose-Estimation-from-a-Single-Depth-Image" class="headerlink" title="A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image"></a>A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</h1><p><em>note at:</em> 202004221543</p>
<p><em>labels:</em> others</p>
<p><strong>conf:</strong> CVPR 2020</p>
<p><strong>author:</strong> Chuang Gan (MIT, MIT-IBM Watson AI Lab)</p>
<p><strong>codes available at:</strong> <a href="http://music-gesture.csail.mit.edu" target="_blank" rel="noopener">project page</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/22/202004221543-CVPR2020-MusicGesture/framework.png" alt="framework"></p>
<h2 id="problems-to-be-solved-task-definition"><a href="#problems-to-be-solved-task-definition" class="headerlink" title="problems to be solved (task definition)"></a>problems to be solved (task definition)</h2><p>问题定义：<br>把不同乐器（不同种，同种不同实例）的音频从视频中各自分离出来</p>
<p>现有方法的问题：</p>
<ul>
<li>虽然现在已经有了一些这个方向的工作，也有工作开始使用visual信息来帮助分离音频。之前的工作都发现了语义信息（semantic appearances）对模型会有帮助，但是这些工作没有考虑视频中的motion cues，这样就限制了这些方法在更复杂的场景下的表现</li>
<li>最近有人利用了temporal motion信息，但是他们基于运动轨迹和光流这样的特征（trajectory and optical flow like features），仍然是局限在对human-object interactions的建模上，没有关注到更细节的地方。</li>
</ul>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><blockquote>
<p>motivation<br>本文中，作者希望利用基于人体关键点的motion cues，同时利用visual feature和sound feature来进行音频分离（sound source separation）</p>
</blockquote>
<blockquote>
<p>method details<br>作者主要在于第一次在这个细分领域使用了关键点信息来表示motion，并且整个work的模型搭建的比较完整，make sense。</p>
</blockquote>
<p>主要贡献在三个地方：</p>
<ul>
<li><p>Video Analysis Network</p>
<ul>
<li><p>这里主要是利用了之前用在其他领域的工作来解决新的问题</p>
</li>
<li><p>利用ResNet50来提取视频的首帧特征，用于做visual context feature</p>
</li>
<li><p>用了ST-GCN（Spatial tempo- ral graph convolutional networks for skeleton-based action recognition）来对提取的人体关键点在时空上建立图模型</p>
</li>
<li><p>利用了alphaPose来提取body关键点，用了一个预训练过的hand detection网络来检测双手，然后利用openpose来提取双手关键点</p>
</li>
<li><p>网络细节：</p>
</li>
<li><p>这里的GCN有11层，并带有residual link。输入是坐标点+置信度(x,y,c),并且在输入网络之前经过了BN层来使得input scale保持相同（这里还需要看看code理解细节！！！）</p>
</li>
<li><p>hand detection网络没有给引用和细节，估计是从git上找了一个</p>
</li>
<li><p>在关键点经过GCN处理后，得到的motion feature又跟之前获得的visual context feature进行了拼接，从而加入了图像信息（这个点有点意思的在于，后面的ablation实验的数据加上这个还不如不加，然而作者并没有给出合理的解释~~）</p>
</li>
</ul>
</li>
<li><p>Audio-Visual Separation Network</p>
<ul>
<li><p>这里作者利用了一个U-Net类型的网络，encoder-decoder结构</p>
</li>
<li><p>encoder输入时2d 音频的频谱</p>
</li>
<li><p>音频信号经encoder得到音频特征后，跟前面的visual+motion feature进行了融合（基于self attention，见下面Audio-visual fusion）</p>
</li>
<li><p>然后fusion feature经过对称的decoder得到最终的audio mask</p>
</li>
<li><p>网络细节：</p>
</li>
<li><p>encoder和decoder对称，都是4层空洞卷积（3 x 3 spatial filters with stride 2, dilation 1 and followed by a BatchNorm layer and a Leaky ReLU）</p>
</li>
</ul>
</li>
<li><p>Audio-visual fusion</p>
<ul>
<li>sound feature和visual feature先经过矩阵乘后softmax来得到attention matrix</li>
<li>然后利用该matrix跟vision feature矩阵乘，再跟sound feature进行相加</li>
<li>得到的结果再经过基于MLP的残差网络得到最终的fusion feature</li>
<li><img src="/2020/04/22/202004221543-CVPR2020-MusicGesture/feature-fusion.png" alt="self attention based Audio-visual fusion"></li>
<li>The MLP is implemented with two fully-connected layers with a ReLU activation function.</li>
</ul>
</li>
</ul>
<p>至于loss什么的细节，文章本身讲的不多，也不是这篇文章的主要关注点，在此略过。</p>
<h2 id="experiments-and-results"><a href="#experiments-and-results" class="headerlink" title="experiments and results"></a>experiments and results</h2><p><img src="/2020/04/22/202004221543-CVPR2020-MusicGesture/result1.png" alt="result1"><br><img src="/2020/04/22/202004221543-CVPR2020-MusicGesture/result2.png" alt="result2"><br>这里的ablation实验也很简单，但是确实有上面提到的问题。</p>
<h2 id="comments"><a href="#comments" class="headerlink" title="comments"></a>comments</h2><p>本文不是自己的主业，看了主要是增多一点视频理解的储备知识。<br>从实验结果上来看，基于keypoints和GCN的方式确实是对这种视频的理解有挺大帮助的。</p>
<h2 id="inspiration"><a href="#inspiration" class="headerlink" title="inspiration"></a>inspiration</h2><p>本文的思路在对手语翻译这个方向上应该来说有很大的借鉴意义，虽然说文中的一些方法已经被别人用过了，也不算什么新方法。不过其中的feature fusion的方式值得学习一波，手语的不同部位可以看做不同的信息源，融合不同部位的信息应该来说是有用的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/22/202004221543-CVPR2020-MusicGesture/" data-id="ck9b7lw8k00004vfy8ijeeszj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CVPR2020/" rel="tag">CVPR2020</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/others/" rel="tag">others</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004211039-BMVC-HMR" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/21/202004211039-BMVC-HMR/" class="article-date">
  <time datetime="2020-04-21T02:39:55.000Z" itemprop="datePublished">2020-04-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/21/202004211039-BMVC-HMR/">202004211039-BMVC-HMR</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Single-Image-3D-Hand-Reconstruction-with-Mesh-Convolutions"><a href="#Single-Image-3D-Hand-Reconstruction-with-Mesh-Convolutions" class="headerlink" title="Single Image 3D Hand Reconstruction with Mesh Convolutions"></a>Single Image 3D Hand Reconstruction with Mesh Convolutions</h1><p><em>note at:</em> 202004211039</p>
<p><em>labels:</em> hand pose mesh</p>
<p><strong>conf:</strong> BMVC 2019(CCF C)</p>
<p><strong>author:</strong> Dominik Kulon</p>
<p><strong>codes available at:</strong> <a href="https://github.com/dkulon/hand-reconstruction" target="_blank" rel="noopener">code link</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/21/202004211039-BMVC-HMR/framework.png" alt="framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><p>首先，这篇文章关注的问题是hand mesh recovery，并且是单人的，也就是说，hand没有scale variation</p>
<p>在此之前，MANO模型解决了这个问题，但是作者提出，SMPL based模型的参数太过，计算比较慢。并且MANO的顶点数太少，对细节表达能力不足</p>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><p>这篇文章描述比之顶会还是差了很远，方法相对来说很简单，也没有很多insight，算是做了一个数据集，一个baseline。（代码公布了，感觉做recovery的话还是可以借鉴一下的）</p>
<blockquote>
<p>main contribution</p>
</blockquote>
<p>作者主要做了三件事</p>
<ul>
<li>做了一个数据集<ul>
<li>具体制作过程在原文第5章</li>
<li>每个mesh有7907个定点，是MANO的十倍还多</li>
<li>包含40360个训练实例，3000个验证实例，3000个测试实例。</li>
<li>标注包括，mesh，3d keypoint（投影到图像坐标系）</li>
</ul>
</li>
</ul>
<ul>
<li>做了一个在hand mesh上的autoencoder，输入mesh，得到mesh<ul>
<li>利用了卷积（看文章的意思，貌似是利用了chebvGCN一样的图卷积）</li>
<li>encoder：输入mesh定点，经过4个卷积层（conv+down sampling+lReLU）</li>
<li>经过一个FC层（64）</li>
<li>decoder：跟encoder对称（这个东西是真正想要的，为了后面从RGB得到mesh而训练）</li>
</ul>
</li>
</ul>
<ul>
<li>single RGB image到相应的hand mesh<ul>
<li>image encoder：利用了DenseNet-121提取图像特征，在imageNet上做了预训练，输出两部分：1)用于mesh decoder的mesh embeding，2)用于回归相机参数的camera embeding</li>
<li>mesh decoder：就是前面autoencoder中的decoder，这里称之为Graph Morphable Model (GMM)——输入为64维隐层特征向量，该向量是image encoder的输出</li>
<li>camera decoder：用于回归得到一个将mesh投影到跟图像的视角一致的相机参数，输入为image encoder得到的camera embedding，经过几层FC层得到相机参数。</li>
<li>loss：mesh上的L1，投影得到的keypoint joints对应的L1，RGB经过image encoder得到的embedding（64维向量）跟mesh autoencoder中encoder对GT mesh进行编码得到的embedding之间的L2。(三个loss的加权系数分别为1，0.01，5e-5，并使用了L2正则，正则系数1e-5)<img src="/2020/04/21/202004211039-BMVC-HMR/loss.png" alt="objective func"></li>
</ul>
</li>
</ul>
<h2 id="discussions"><a href="#discussions" class="headerlink" title="discussions"></a>discussions</h2><p>这篇文章思路很简单，主要功夫在做数据集上，但是这个数据集又有些局限。并且文章叙述确实是不好，很多地方不太清楚，同时实验部分不清晰，实验结果甚至都看不到，论证的点很多没有相应的实验佐证。</p>
<p>但无论如何，开源了代码，作为一个baseline的参考还是有价值的。</p>
<h2 id="available-datasets"><a href="#available-datasets" class="headerlink" title="available datasets"></a>available datasets</h2><p>需要通过发送邮件来跟作者要</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/21/202004211039-BMVC-HMR/" data-id="ck99ozik50000b9fy4lgj58hu" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hand-pose-mesh/" rel="tag">hand pose mesh</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/others/" rel="tag">others</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004151649-ICCV2019-A2J" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/20/202004151649-ICCV2019-A2J/" class="article-date">
  <time datetime="2020-04-20T09:59:34.000Z" itemprop="datePublished">2020-04-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/20/202004151649-ICCV2019-A2J/">202004151649-ICCV2019-A2J</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="A2J-Anchor-to-Joint-Regression-Network-for-3D-Articulated-Pose-Estimation-from-a-Single-Depth-Image"><a href="#A2J-Anchor-to-Joint-Regression-Network-for-3D-Articulated-Pose-Estimation-from-a-Single-Depth-Image" class="headerlink" title="A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image"></a>A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</h1><p><em>note at:</em> 202004151649</p>
<p><em>labels:</em> 3d hand pose esti depth</p>
<p><strong>conf:</strong> ICCV 2019</p>
<p><strong>author:</strong> YUAN Junsong</p>
<p><strong>codes available at:</strong> <a href="https://github.com/zhangboshen/A2J" target="_blank" rel="noopener">code link</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/20/202004151649-ICCV2019-A2J/framework1.png" alt="main idea"></p>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/framework2.png" alt="framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><ul>
<li>现有基于encoder-decoder FCN方式的工作基本上都利用gaussian heatmap这种non-adaptive的训练方式，其计算代价大，并且难以end2end</li>
<li>3D CNN的方式需要训练的参数多，并且体素化操作比较麻烦且开销大</li>
<li>基于point-set的方式则需要花费很多时间在数据预处理上</li>
</ul>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><blockquote>
<h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3></blockquote>
<p>借鉴anchor based方式，利用anchor来获得keypoints坐标。key idea是：通过多个预测anchor的偏移，然后加权求和从而获得3d pose预测，利用整体学习来获得更好的泛化性。(The key idea of A2J is to predict 3D joint position by aggregating the estimation results of multiple anchor points, in spirit of en- semble learning to enhance generalization ability.)</p>
<h3 id="method-details"><a href="#method-details" class="headerlink" title="method details"></a>method details</h3><ul>
<li>ResNet50 做backbone来提取特征，前三层提取common feature，最后一层提取semantic feature</li>
<li>in-plain offset estimation branch来获得anchor的offset</li>
<li>depth estimation branch来获得关键点的深度</li>
<li>anchor proposal branch来获得加权权重</li>
</ul>
<blockquote>
<p>in-plain offset estimation branch</p>
</blockquote>
<ul>
<li>input: 从backbone提取的semantic feature(trunk), </li>
<li>经过4个conv2d + BN + ReLU，</li>
<li>最后经过一个output conv（16 x K x 2 通道）得到offsets，</li>
<li>这里的16代表，每个feature point代表了16个anchor points（因为input是image经过16倍下采样的，而每个anchor是在image上每隔4个point取一个做anchor set）</li>
</ul>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/in-plain.png" alt="in-plain and depth estimation"></p>
<blockquote>
<p>depth estimation branch</p>
</blockquote>
<ul>
<li>前边跟in-plain branch一样</li>
<li>最后经过一个output conv（16 x K x 1 通道）得到depth</li>
</ul>
<blockquote>
<p>anchor proposal branch</p>
</blockquote>
<ul>
<li>input: 从backbone提取的common feature(trunk),</li>
<li>同样经过4个conv2d + BN + ReLU，</li>
<li>最后经过一个output conv（16 x K x 1 通道）得到加权值</li>
</ul>
<blockquote>
<h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3></blockquote>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/loss1.png" alt="loss1"><br><img src="/2020/04/20/202004151649-ICCV2019-A2J/loss2.png" alt="loss2"></p>
<p>loss = 3loss1 + loss2</p>
<h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><blockquote>
<p>comparison with SOTA</p>
</blockquote>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/comparision.png" alt="comparison with SOTA"></p>
<blockquote>
<p>ablation</p>
</blockquote>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/ablation.png" alt="ablation"></p>
<h2 id="discussions"><a href="#discussions" class="headerlink" title="discussions"></a>discussions</h2><h2 id="available-datasets"><a href="#available-datasets" class="headerlink" title="available datasets"></a>available datasets</h2><p>NYU: </p>
<p><a href="https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm#download" target="_blank" rel="noopener">website</a> </p>
<p><a href="http://horatio.cs.nyu.edu/mit/tompson/nyu_hand_dataset_v2.zip" target="_blank" rel="noopener">data</a></p>
<h2 id="some-inspirations"><a href="#some-inspirations" class="headerlink" title="some inspirations"></a>some inspirations</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/20/202004151649-ICCV2019-A2J/" data-id="ck98baczg0000kifyecrkhiuk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ICCV2019/" rel="tag">ICCV2019</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hand-pose-depth/" rel="tag">hand pose depth</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004060009-ICCV2019-HAMR" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/18/202004060009-ICCV2019-HAMR/" class="article-date">
  <time datetime="2020-04-18T12:18:00.000Z" itemprop="datePublished">2020-04-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/18/202004060009-ICCV2019-HAMR/">202004060009-ICCV2019-HAMR</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="End-to-end-Hand-Mesh-Recovery-from-a-Monocular-RGB-Image"><a href="#End-to-end-Hand-Mesh-Recovery-from-a-Monocular-RGB-Image" class="headerlink" title="End-to-end Hand Mesh Recovery from a Monocular RGB Image"></a>End-to-end Hand Mesh Recovery from a Monocular RGB Image</h1><p><em>note at:</em> 202004060009</p>
<p><strong>conf:</strong> ICCV 2019</p>
<p><strong>author:</strong> Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, Wen Zheng</p>
<p><strong>codes available at:</strong> <a href="https://github.com/Wavelet303/HAMR" target="_blank" rel="noopener">code link</a>(only inference code available)</p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/18/202004060009-ICCV2019-HAMR/HAMR-framework.png" alt="HAMR-framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><ul>
<li>single RGB 图像缺少深度信息，对于hand这种自遮挡较严重，表达复杂多样的任务，具有较大难度。</li>
<li>数据标注难度大，目前有几个人工合成的数据，但是合成数据跟真实的数据之间是存在gap的</li>
</ul>
<h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>利用参数化hand model，从single RGB通过端到端方式重建出对应的hand mesh，利用这一更高维的表达可以进一步获得低维的2d，3d pose。</p>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><p>总体来讲，本文的思路就是利用2d pose estimation的工作作为head，提取出RGB图像的特征并得到2d joints的heatmap，然后利用heatmap和图像特征，进一步通过网络迭代的学习到相机参数和mesh参数，进而利用参数获得mesh。</p>
<p>具体来说：</p>
<p>对于2d pose estimation部分：<br>作者利用了经典的hourglass网络，但做出了一些调整：</p>
<ul>
<li>使用3*3卷积替代了原网络中的residual module</li>
<li>average pooling替换成了原网络的max pooling</li>
<li>在每个3*3卷积后增加了BN层</li>
<li>不止获得21个joints heatmap，增加了一些冗余来增加特征</li>
</ul>
<p>对于参数回归（Iterative Regression Module）：<br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/regression.png" alt="Iterative Regression Module"></p>
<ul>
<li>输入包括joints heatmap，冗余feature maps，hourglass的中间层feature maps</li>
<li>网络结构是简单的全卷积+三层全连接层</li>
<li>最后输出两个参数（相机参数，mesh参数）</li>
<li>整个过程是迭代进行的</li>
</ul>
<p>其中相机参数（s, tx, ty）由三个数组成。GT——在知道2d，3d坐标后，可以反向求解得到该参数。<br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/coord2K.png" alt="coord2K"><br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/3d-2d.png" alt="3d-2d"></p>
<p>mesh参数（MANO参数）：</p>
<ul>
<li>β： shape参数，10维，控制hand的具体shape信息，如胖瘦，手指粗细等，是一种PCA的参数</li>
<li>$\theta$：pose参数，K*3维，控制手的姿态，可以理解为K个关键点用Rodrigues向量表示下的3D旋转</li>
<li>根据这两个参数，MANO模型可以生成相应的hand mesh</li>
</ul>
<h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3><ul>
<li>heatmap loss：pixel wise distance（应该是逐像素L1）</li>
<li>2d loss：L2 loss</li>
<li>3d loss：L2 loss</li>
<li>geometry loss：每只指头4关节共面&amp;&amp;每只手指的三个骨头相对旋转角度方向一致，即向量表示的向量积&gt;0</li>
<li>camera loss：相机内参loss，L2 loss</li>
<li>seg loss：mesh的投影跟hand的GT silhouette之间的 L1 loss</li>
</ul>
<h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><p><img src="/2020/04/18/202004060009-ICCV2019-HAMR/result1.png" alt="coord2K"><br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/result2.png" alt="3d-2d"><br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/result3.png" alt="coord2K"></p>
<h2 id="discussions"><a href="#discussions" class="headerlink" title="discussions"></a>discussions</h2><h2 id="some-inspirations"><a href="#some-inspirations" class="headerlink" title="some inspirations"></a>some inspirations</h2><p>本文算是很直接的利用MANO模型的例子，没有很复杂的网络结构，思路也非常直接，总体来说亮点不算很明显，主要在于做的早，很快的把人体的工作迁移到了hand上。</p>
<p>值得注意的是，本文同样是尽可能的利用了各种能用的监督信息。这一点在hand任务中应该确实是挺重要的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/18/202004060009-ICCV2019-HAMR/" data-id="ck95mfta40000onfy46z935ux" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ICCV2019/" rel="tag">ICCV2019</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hand-pose-mesh/" rel="tag">hand pose mesh</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/28/hello-world/" class="article-date">
  <time datetime="2020-03-28T15:42:36.035Z" itemprop="datePublished">2020-03-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/28/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/03/28/hello-world/" data-id="ck95mfta90002onfy0sexa3l0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR2020/" rel="tag">CVPR2020</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICCV2019/" rel="tag">ICCV2019</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SLR/" rel="tag">SLR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hand-pose-depth/" rel="tag">hand pose depth</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hand-pose-mesh/" rel="tag">hand pose mesh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/others/" rel="tag">others</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CVPR2020/" style="font-size: 20px;">CVPR2020</a> <a href="/tags/ICCV2019/" style="font-size: 20px;">ICCV2019</a> <a href="/tags/SLR/" style="font-size: 10px;">SLR</a> <a href="/tags/hand-pose-depth/" style="font-size: 10px;">hand pose depth</a> <a href="/tags/hand-pose-mesh/" style="font-size: 20px;">hand pose mesh</a> <a href="/tags/others/" style="font-size: 20px;">others</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/23/202004232043-CVPR2020-SLT/">202004232043-CVPR2020-SLT</a>
          </li>
        
          <li>
            <a href="/2020/04/22/202004221543-CVPR2020-MusicGesture/">202004221543-CVPR2020-MusicGesture</a>
          </li>
        
          <li>
            <a href="/2020/04/21/202004211039-BMVC-HMR/">202004211039-BMVC-HMR</a>
          </li>
        
          <li>
            <a href="/2020/04/20/202004151649-ICCV2019-A2J/">202004151649-ICCV2019-A2J</a>
          </li>
        
          <li>
            <a href="/2020/04/18/202004060009-ICCV2019-HAMR/">202004060009-ICCV2019-HAMR</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>