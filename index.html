<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://zhang-mohole.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zhang-mohole.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-202005072249-ARXIV2020-BMVC-SLP-transformer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/" class="article-date">
  <time datetime="2020-05-07T14:50:25.000Z" itemprop="datePublished">2020-05-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/">202005072249-ARXIV2020_BMVC-SLP_transformer</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Progressive-Transformers-for-End-to-End-Sign-Language-Production"><a href="#Progressive-Transformers-for-End-to-End-Sign-Language-Production" class="headerlink" title="Progressive Transformers for End-to-End Sign Language Production"></a>Progressive Transformers for End-to-End Sign Language Production</h1><p><em>note at:</em> 202005072249</p>
<p><em>labels:</em> SLP</p>
<p><strong>conf:</strong> maybe BMVC 2020</p>
<p><strong>author:</strong> Ben Saunders, Necati Cihan Camgoz, and Richard Bowden (University of Surrey)</p>
<p><strong>no codes available:</strong> </p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/framework.png" alt="framework overview"></p>
<h2 id="main-contribution"><a href="#main-contribution" class="headerlink" title="main contribution"></a>main contribution</h2><p>本文是第一次把transformer结构用在SLP问题上</p>
<p>并且，这篇文章非常重要的一点是，在decoder部分提出了一个counter encoding的方式，来解决少量文本词汇生成大量视频帧（这里是pose seq）这一过程中，需要生成多少帧（视频持续时间），什么时候停止这一棘手的问题。这个有点意思。</p>
<h2 id="method-proposed"><a href="#method-proposed" class="headerlink" title="method proposed"></a>method proposed</h2><p>本文的主体结构基本上就是一个transformer，只有两点稍有不同：</p>
<ul>
<li>在decoder部分的输入输出是pose skeleton + counter encoding（即关键点3d坐标+counter encoding）</li>
<li>在decoder部分的self attention模块和feed forward层之间又加了一层encoder-decoder multi-head attention，用于更好的编码text信息和gloss/pose seq信息的相互关系。</li>
</ul>
<p><img src="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/net-architecture.png" alt="network architecture"></p>
<blockquote>
<p>network architecture 解释</p>
</blockquote>
<p>symbolic embedding就是一个word2vec操作，就是一个简单的MLP/FC层对text或者gloss的one hot编码进行转换，转化到一个向量空间，只不过这里并没有单独训练这个小的网络。</p>
<p>temporal embedding就是跟BERT里一样的东西，就是一个positional embedding</p>
<blockquote>
<p>counter encoding</p>
</blockquote>
<p>在生成pose seq的过程，输入输出都是3D pose coordinates，</p>
<p>3D pose coordinate本质上就是一个向量，跟one hot编码没有大的区别，只不过坐标空间是个连续空间。所以这里的输入输出跟encoder的不同只是离散空间和连续空间的区别，正是这个区别使得decoder在最后不需要进行softmax来获取多分类概率从而得到目标词汇，而是直接回归得到。不过这里也不是把3D pose coordinate直接就输入给transformer了，而是跟前面的symbolic embedding一样，也加了一层embedding把坐标空间进一步转化到一个dense latent space</p>
<p>最重要的是，这里需要进行一个counter encoding</p>
<ul>
<li>counter encoding就是一个标记位置的功能，通过step/N得到一个分数（小数）直接获得一个非常简单的编码</li>
<li>这个小数恰恰反映了当前生成的pose在整句话所处的位置，也就是说现在生成到整个过程的多少了（进度条）</li>
<li>同时，也是因为这个counter encoding具有这样的记录位置的功能，那么当它到1的时候，就是整个句子生成结束的时候，所以也就不需要一个特殊的符号或者pose来表示句子结束了。（这个编码确实是非常简单巧妙）<br><img src="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/couter-encoding.png" alt="counter encoding"></li>
</ul>
<blockquote>
<p>GT pose</p>
</blockquote>
<p>ground truth pose是通过openpose先获得2d坐标，然后又利用一个现有的方法直接将2d坐标lift到了3d coordinate。具体细节文中没讲，可以再看看引文。<br><img src="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/skeleton-extract.png" alt="3D ground truth skeleton extracting"></p>
<h2 id="expriments-and-results"><a href="#expriments-and-results" class="headerlink" title="expriments and results"></a>expriments and results</h2><blockquote>
<p>implementation details</p>
</blockquote>
<p>transformer 使用了2层的结构；MHA用了8 head；embedding size=256；利用Xavier initialisation进行初始化，Adam优化器，lr=0.001；基于JoeyNMT tool kit实现<a href="https://github.com/joeynmt/joeynmt" target="_blank" rel="noopener">joeynmt git link</a></p>
<p>除了生成，作者利用反过来的过程，pose seq-&gt;text（引文6 Camgoz, N.C., Hadfield, S., Koller, O., Bowden, R., Ney, H.: Neural Sign Language Translation.）的方式反向得到text，然后在得到的text上进行评估（BLEU 1-4 score）</p>
<p><img src="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/result1.png" alt="result"></p>
<p><img src="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/self-comparing.png" alt="text2poses v.s. text2gloss2poses"></p>
<h2 id="some-comments"><a href="#some-comments" class="headerlink" title="some comments"></a>some comments</h2><p>本文基本上是把自己想做的给做出来了<del>~</del>，不过还没有生成真正的图片，只是做了text-&gt;pose这一步，但是感觉作者又是在憋大招，这个小工作发一篇小论文，后面继续优化图像生成部分，以后投更高水平的工作，有一定压力了。</p>
<p>总体来说，本文的复现是可行的，接下来可以进行尝试。</p>
<p>不过这篇文章也存在明显的不足：</p>
<ul>
<li>这样直接生成pose seq，可以肯定最后的效果只是大概很像，但实际上肯定不够精确，毕竟手语对手部和相对位置的要求是很高的，这样的方法也就只能生成一些大致的结果，细节肯定不到位</li>
<li>对于counter encoding，作者提出了第一种方法，但是这个方法太过简单，只有一个数字来进行编码，在实际使用过程中，回归出来的数字应该不会很好用，尤其是用它来控制生成过程的结束，这一点可以进行改进</li>
<li>论文中提到counter encoding在inference的时候也是直接给出N的，这就有点说不过去，这相当于直接知道了要生成多少帧，这讲不通。实际上我们关注的是动作，手指细节而不是具体有多少帧</li>
<li>得到的pose是一种非常稀疏的表示，没有其他手语需要关注的信息输出（如表情）</li>
</ul>
<h2 id="inspiration"><a href="#inspiration" class="headerlink" title="inspiration"></a>inspiration</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/" data-id="ck9wyd2sm0000z4fy3k0i9pfl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SLT/" rel="tag">SLT</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202005042101-CVPR2020-PhotometricH-O" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/" class="article-date">
  <time datetime="2020-05-04T13:01:04.000Z" itemprop="datePublished">2020-05-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/">202005042101-CVPR2020-PhotometricH+O</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Leveraging-Photometric-Consistency-over-Time-for-Sparsely-Supervised-Hand-Object-Reconstruction"><a href="#Leveraging-Photometric-Consistency-over-Time-for-Sparsely-Supervised-Hand-Object-Reconstruction" class="headerlink" title="Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction"></a>Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction</h1><p><em>note at:</em> 202005042101</p>
<p><strong>conf:</strong> CVPR 2020</p>
<p><strong>author:</strong> Yana Hasson</p>
<p><strong>codes available at:</strong> <a href="https://hassony2.github.io/handobjectconsist" target="_blank" rel="noopener">code link</a>(on the way…)</p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/framework.jpeg" alt="framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><ul>
<li>手跟物体交互场景下手跟物体的互遮挡现象严重</li>
<li>3d数据标注成本非常高，当前数据集在该任务上难以做全监督的学习<ul>
<li>motion capture datasets:虽然可以提供大量具有精确标注的训练样本，但是这些数据只能在特定条件下采集，并且会有可见的传感器之类的东西，这些出现在rgb image中会影响模型</li>
<li>multi-view setups:虽然可以通过multiview来获得3d信息，但是这种数据也是对场景（采集条件）有特定要求</li>
<li>synthetic datasets:虽然是一种方法，但是目前而言，合成数据仍然存在domain gap的问题</li>
<li>手工标注以及optimization-based方式:太费时间</li>
</ul>
</li>
</ul>
<blockquote>
<p>motivation</p>
</blockquote>
<p>基于以上问题，本文提出了一种新的思路：利用视频数据在时间维度上的光度测量一致性(photometric consistency)，在数据集中只有少量子数据集有3d标注时可以进行从single RGB到hand and object基于弱监督（自监督）的重建。</p>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><p>本文在主体框架上思路比较简单，就是一个encoder对图像进行特征提取，然后利用提取到的特征进行hand的pose+shape参数回归（基于MANO模型）以及object的6d pose回归</p>
<blockquote>
<p>reconstruction</p>
</blockquote>
<p>网络结构即上文中的框架，利用了非常简单的ResNet-18作为encoder(除去最后一层分类层)提取图像特征<br>接下来decoder部分分为三个部分：</p>
<ul>
<li>MANO based hand reconstruction，目标是获得28维MANO参数(3 global rotation,15 pose, 10 shape)</li>
<li>object的6d pose estimation。 mesh已知，目标是获得6维object pose参数</li>
<li>global hand translation参数回归。目标是获得3维全局转换向量，用于得到点在世界坐标系中的坐标。</li>
<li><img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/decoder.jpeg" alt="decoder net details"></li>
<li>数据处理上，作者利用了一种基于相机焦距的normalize方法：<img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/focal-normalize.jpeg" alt="focal normalize">，在已知这里的d_f, (t_u, t_v)以及相机内参后，可以得到global translation。（这里还有一个global rotation，论文只有一句话，没看明白是要干什么？）</li>
</ul>
<blockquote>
<p>photometric consistency loss</p>
</blockquote>
<p>本文中作者最核心的创新在于提出的photometric consistency loss，并利用该loss在该任务上进行了自监督学习。<br><img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/photometric-loss.jpeg" alt="photometric consistency loss"></p>
<p>该loss的设计思想是，给一个ref image: I_ref，它带有3d标注，要进行重建的image为I_ref+k。那么在ref+k重建后，将重建得到的vetices投影到image 空间，给每个vertices赋予在image空间上对应点的color值，因为物体和手都是同一个，那么ref+k经过这种投影赋值的结果应该跟ref相应的值一样。把这种想法设计成了一种loss。</p>
<p>具体步骤（跟整体思想稍有不一样）：</p>
<ul>
<li>首先获得ref+k的reconstruction；计算ref到ref+k相应vertices的3d displacement(“flow”)，然后将之投影到image空间（在mesh的可见区域进行插值），这里利用了引文[20]”Neural Renderer”。这样就建立了从ref到ref+k之间的warping flow，即一个矩阵W1(ref-&gt;ref+k)。反过来，从ref+k到ref的warping map也可以得到，W2。</li>
<li>然后利用该W以及一个visible mask来在image上得到loss：<img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/loss-caculate.jpeg" alt="photometric loss"></li>
<li>visible mask可以通过对ref的mesh进行投影得到。在计算loss时，需要保证参与计算loss的pixel在两个image中都是可见的，所以这里也用了cyclic consistency check方法。该方法先利用W1将refwarp到(ref+k)’，然后再用W2将得到的(ref+k)’再warp回到(ref)’，如果ref和(ref)’之间的差距超过2pixel，则该像素不参与计算（这个比例在实验中非常小）。通过这种手段使得参与loss计算的像素在两帧图像中都是稳定可见的。</li>
<li>(这个loss的一些细节没有看懂，这里存在两个引文的盲区[20], cyclic consistency check-[14,29])</li>
</ul>
<h2 id="expriments-and-results"><a href="#expriments-and-results" class="headerlink" title="expriments and results"></a>expriments and results</h2><p><img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/result1.jpeg" alt="comparing with SOTA"></p>
<p><img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/result3.jpeg" alt="PCK and weakly supervised"></p>
<p><img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/result4.jpeg" alt="ablation"></p>
<p><img src="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/result2.jpeg" alt="qualitative results"></p>
<h2 id="some-comments"><a href="#some-comments" class="headerlink" title="some comments"></a>some comments</h2><p>本文总体来说模型框架很简单，但是这个photometric loss是一个很有趣的点，使得任务可以在弱监督下进行。</p>
<p>论文中对于实现的细节，尤其是该loss的实现细节描述不够，还需要等代码开源后进一步查看。</p>
<p>文中针对不同数据集上joint定义不同对于hand shape的影响，提出了利用skeleton adaptation layer（就是一个全连接层，用于把mesh vertices回归到joints）替换MANO的joint regression layer。 这一改动的具体细节需要特别关注代码的具体实现。</p>
<h2 id="inspiration"><a href="#inspiration" class="headerlink" title="inspiration"></a>inspiration</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/05/04/202005042101-CVPR2020-PhotometricH-O/" data-id="ck9sm7ycq000042fy526f7ysf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CVPR2020/" rel="tag">CVPR2020</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hand-object/" rel="tag">hand+object</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004080031-ECCV2018-2-5D-heatmap" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/" class="article-date">
  <time datetime="2020-04-29T13:32:56.000Z" itemprop="datePublished">2020-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/">202004080031-ECCV2018-2.5D_heatmap</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Hand-Pose-Estimation-via-Latent-2-5D-Heatmap-Regression"><a href="#Hand-Pose-Estimation-via-Latent-2-5D-Heatmap-Regression" class="headerlink" title="Hand Pose Estimation via Latent 2.5D Heatmap Regression"></a>Hand Pose Estimation via Latent 2.5D Heatmap Regression</h1><p><em>note at:</em> 202004080031</p>
<p><em>labels:</em> hand pose est RGB</p>
<p><strong>conf:</strong> ECCV 2018</p>
<p><strong>author:</strong> Umar Iqbal, Pavlo Molchanov, Thomas Breuel Juergen Gall, Jan Kautz</p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/framework.png" alt="framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><ul>
<li>single RGB -&gt; 3d hand pose的工作不多。从2d图像到3d pose，缺乏深度信息和scale信息，是一个ill-posed task，之前的方法要么无法解决，要么是在特定情境的数据下过拟合。</li>
<li>制作一个大规模的多场景数据集是难以行得通的。</li>
<li>之前的方法需要有一些前置假设，如palm的坐标已知，hand scale已知；要么就是只能得到一个大概的结果。</li>
</ul>
<h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3><p>利用可学习的方式将RGB image编码到2.5D（2d heatmap + depth map），进而得到3D pose。利用2.5D表示，同时解决掉尺度不变性和平移不变性（scale and translation invariant）——这里的2.5D，本质上就是一个RGB+depth的表示嘛。</p>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><p>本文最重要的工作在两个点上</p>
<ul>
<li>2.5D heatmap的定义</li>
<li>2.5D heatmap的获得</li>
</ul>
<p>2.5D heatmap定义：</p>
<p>主要目标在于利用一种有着2d heatmap的紧凑性（节约计算量）和3d heatmap的语义性的heatmap表示。既能方便表达位置信息，又能便于计算。</p>
<p>K个关键点的2.5D heatmap由2K个h*w的heatmap组成，其中K个就是uv空间的2d joints hm，剩下K个则代表深度上的数值。</p>
<ul>
<li>2d 部分跟之前工作中用到的一样，表示了joints在uv空间的位置信息</li>
<li>depth的表示，这里采用的是相对于root joint的相对深度，那么在绝对空间中的深度是depth_root + depth_r，这里的depth hm上仅存储depth_r。</li>
<li>对于depth，还进行了normalization，从而保证了这个2.5D表示的平移不变性和尺度不变性:<img src="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/normalization.png" alt="normalization"></li>
<li>这里的s（scale）在2.5D恢复到相机空间3d pose时需要，如果已知就直接用，如果这个量不知道时:<img src="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/scale.png" alt="scale"></li>
</ul>
<p>2.5D 到 3D pose</p>
<p>有了上述2.5D表示，可以很方便的获得uv坐标和depth_r，给出depth_root之后，利用相机参数即可恢复出在相机空间中的3d坐标。</p>
<p>2.5D heatmap的获得</p>
<p>有两种方式：<br>1、通过一种很直接的方式；2、通过可学习的方式来获得adaptive latent 2.5D heatmap。</p>
<p>直接方式：</p>
<ul>
<li>对于2d heatmap，跟经典方法一样，直接通过高斯分布获得K个h*w的heatmap H_2d</li>
<li>对于depth heatmap，直接用H_2d点乘相应joint的GT depth_r</li>
</ul>
<p>直接方式有一些缺点：1、对于2d heatmap，在手势中，每个joint产生的热力区域都是一样的，但实际上不同位置对于热力区域大小的需要不同，手掌更大，手指则应该更小。这种语义并没有在直接方式中获得；2、depth heatmap则有更大的问题，直接点乘H_2d，相当于直接利用了高斯分布，但是高斯分布是非常陡峭的，而实际上手的深度变化不会那么大，如手掌区域的depth值实际上差别很小，直接方式会破坏这种语义。</p>
<p>基于以上缺点，作者提出了：</p>
<p>Latent 2.5D Heatmap Regression:</p>
<ul>
<li>利用一个CNN来编码获得2K个hm，分别是2d和depth heatmap，然后再对他们分别进行处理获得所需要的2.5D heatmap。</li>
<li>对于2d heatmap，利用空间softmax把heatmap上的值进行概率化，同时加入科学系参数β来自适应的学习每个关节的热力区域大小的权重，从而得到概率矩阵，利用该概率矩阵可以进一步得到joint的2d坐标：<img src="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/latent2dhm.png" alt="latent2dhm"></li>
<li>对于depth heatmap，利用刚才上一步得到的概率矩阵跟CNN得到的depth map进行哈德马乘，所得即为depth_r:<img src="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/latent_depth_hm.png" alt="latent_depth_hm"></li>
<li>通过这种方式，整个过程是可导的，所以可以通过学习方式获得2.5D heatmap的编码，同时还能解决上面直接编码的不足之处。</li>
</ul>
<h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><p>ablation result:<br><img src="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/ablation.png" alt="ablation"></p>
<p>Comparison with the state-of-the-art:<br><img src="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/sota-compare.png" alt="compare with SOTA"></p>
<h2 id="discussions"><a href="#discussions" class="headerlink" title="discussions"></a>discussions</h2><p>直接利用2d来lifting的方式效果是比较一般的，这篇文章这种2.5D heatmap的方法一定程度上更充分的利用了2d heatmap的优势，是一个有趣的思路，不过其性能应该也不会特别好。</p>
<h2 id="some-inspirations"><a href="#some-inspirations" class="headerlink" title="some inspirations"></a>some inspirations</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/" data-id="ck9lel2xb0000ukfyayg23abq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ECCV2018/" rel="tag">ECCV2018</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hand-pose-RGB/" rel="tag">hand pose RGB</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004271600-BMVC2018IJCV2020-SLP" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/" class="article-date">
  <time datetime="2020-04-27T08:00:23.000Z" itemprop="datePublished">2020-04-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/">202004271600-WACV2018IJCV2020-SLP</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="SignLanguage-Production-Using-Neural-Machine-Translation-And-Generative-Adervsarial-Networks"><a href="#SignLanguage-Production-Using-Neural-Machine-Translation-And-Generative-Adervsarial-Networks" class="headerlink" title="SignLanguage Production Using Neural Machine Translation And Generative Adervsarial Networks"></a>SignLanguage Production Using Neural Machine Translation And Generative Adervsarial Networks</h1><p><em>note at:</em> 2020-04-27 16:00</p>
<p><em>labels:</em> SLP</p>
<p><strong>conf:</strong> BMVC 2020</p>
<p><strong>author:</strong> Stephanie Stoll, Necati Cihan Camgoz, (Centre for Vision, Speech and Signal Processing University of Surrey)</p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/bmvc-framework.png" alt="first NN based SLP framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><ul>
<li>之前SLP相关的工作大多是基于avatar的</li>
<li>一种方式是利用motion capture data来驱动avatar做出相应的动作，这种方式需要建立一个pre-recorded phrases相应的动作驱动表示，这种基于motion capture的方式的代价很高</li>
<li>另一种方式的思路是：把text翻译为对应的sign gloss(一个手语词汇的表示)，然后建立该 gloss entity的parametric representation，例如驱动avatar用到的hand shape和motion。这种方式的问题在于：1、这种直接逐词翻译是不符合手语的语法的；2、对于手语中的其他表示信息，这里忽略了（如表情之类的）；最终最好也就能得到粗略的表示，更甚只能得到错误的手语结果。</li>
</ul>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><p>本文是第一次使用DL的方式来解决text2real-person-sign-video任务的，第一次定义了这个问题，并提出了自己的解决方法。利用SOTA的NMT模型来进行text2glosses翻译，建立glosses到对应upper body pose sequence的lookup table获得gloss到pose sequence的映射，最终通过一个VAE+DCGAN模型逐帧生成相应的sign video。</p>
<blockquote>
<p>method details</p>
</blockquote>
<ul>
<li><p>text to gloss translation</p>
<ul>
<li>这里使用了经典的encoder-decoder with Luong attention</li>
<li>具体的网络结构是，encoder decoder使用了4层GRU，隐层特征单元为1000维</li>
<li>Adam, lr:1e-5 30 epochs, dropout:0.2</li>
</ul>
</li>
<li><p>gloss to skeletal mapping</p>
<ul>
<li>这里的mapping是从一个gloss sentence映射到一个skeletal sequence</li>
<li>其中关键点是利用openpose提取的，这里只用了upper body部分(10个)，并没有使用hand pose</li>
<li>对skeletal的处理：<ul>
<li>把reference image中的pose作为ref，先把sequence中所有的skeletal对齐到跟ref的neck相同位置（先用ref的neck坐标减去输入的neck坐标得到neck的偏差，然后将输入的skeletal坐标都加上这个数，这样就得到了输入skeletal平移到ref位置的坐标）<img src="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/bmvc-align-neck.jpeg" alt="first step"></li>
<li>根据肩膀宽度来计算输入skeletal跟ref skeletal之间的比例因子f（ref的左右肩膀坐标差，除以输入skeletal的左右肩膀坐标差）<img src="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/bmvc-factor.jpeg" alt="second step"></li>
<li>最后，利用经过neck关键点对齐的skeletal以及比例因子f，计算normalized skeletal。（1、用第一步计算得到的输入经过平移得到的neck对齐后的结果减去ref的neck坐标，这样得到了该skeletal各关键点相对于neck的相对坐标，然后该相对坐标乘以f得到跟ref同比例的skeletal相对坐标，最后加上ref的neck坐标变成跟red的neck位置相同的绝对坐标）<img src="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/bmvc-skeletal-norm.jpg" alt="third step"></li>
<li><img src="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/bmvc-gloss-representation.jpeg" alt="representative mean sequence for a gloss"></li>
</ul>
</li>
</ul>
</li>
<li><p>pose conditioned sign generation network</p>
<ul>
<li>image encoder and generator<ul>
<li>输入是ref image(128x128x3) + upper body skeletal对应的heatmap(128<em>128</em>10)；输出是跟该skeletal对应的image(128<em>128</em>3)</li>
<li>encoder由conv和FC组成，5次conv，每次conv前先将feature maps跟resize到相应大小的heatmap进行concat；</li>
<li>decoder的结构跟encoder结构对称，不过没有FC层，使用deconv，每次deconv之前跟encoder相应位置的特征之间有residual link</li>
<li><img src="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/bmvc-encoder-decoder.png" alt="encoder decoder network details"></li>
</ul>
</li>
<li>discriminator<ul>
<li>其详细结构论文中没有提及，只说了输入输出</li>
<li>input：1、G得到的生成图像/GT图像；2、skeletal pose heatmap；3、base pose input image（ref image，用于在训练multi signer时鉴别G得到的人跟ref是否是一个）</li>
</ul>
</li>
<li>loss<ul>
<li><img src="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/bmvc-loss.jpeg" alt="loss"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="discussions"><a href="#discussions" class="headerlink" title="discussions"></a>discussions</h2><p>这篇文章看来是开山之作了，虽然存在一些问题，但是无疑已经把这半年想要抢第一个挖坑机会的想法给覆灭了<del>~</del></p>
<ul>
<li>其中存在的问题主要在于，这里的pose作者仅仅使用了upper body，对于hand pose这一很重要的信号，并没有在GAN网络中用到，这里必然导致最终生成的质量不好，语义表达一定是不太好的。</li>
<li>再者，在gloss到pose sequence这一步，作者给出的方法是给一个lookup table来进行mapping，这里方法部分介绍的不多，但从其中的意思可以看出是用一个从GT skeletal中计算得到的向量来做key，从而在得到一个loss后利用这样一种representation来进行映射得到pose sequence。首先这种预定义的库的确是一种可行的思路，但是这种思路确实是很strate forward，应该有更好的可以end2end的方法。</li>
</ul>
<h2 id="available-datasets"><a href="#available-datasets" class="headerlink" title="available datasets"></a>available datasets</h2><p>the SMILE Sign Language Assessment Dataset: Smile swiss german sign language dataset, 02 2018.(去官方网页看并没有一个下载链接，连数据都看不到，可以发邮件问问)</p>
<h2 id="some-inspirations"><a href="#some-inspirations" class="headerlink" title="some inspirations"></a>some inspirations</h2><h1 id="Text2Sign-Towards-Sign-Language-Production-Using-Neural-Machine-Translation-and-Generative-Adversarial-Networks"><a href="#Text2Sign-Towards-Sign-Language-Production-Using-Neural-Machine-Translation-and-Generative-Adversarial-Networks" class="headerlink" title="Text2Sign: Towards Sign Language Production Using Neural Machine Translation and Generative Adversarial Networks"></a>Text2Sign: Towards Sign Language Production Using Neural Machine Translation and Generative Adversarial Networks</h1><p><em>note at:</em> 202001121635</p>
<p><em>labels:</em> SLP</p>
<p><strong>journal:</strong> IJCV 2020</p>
<p><strong>author:</strong> Stephanie Stoll , Necati Cihan Camgoz</p>
<p><strong>codes available at:</strong> <a href="https://github.com/3d-hand-shape/hand-graph-cnn" target="_blank" rel="noopener">code link</a></p>
<h2 id="framework-1"><a href="#framework-1" class="headerlink" title="framework"></a>framework</h2><h2 id="problems-to-be-solved-1"><a href="#problems-to-be-solved-1" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><h2 id="methods-proposed-1"><a href="#methods-proposed-1" class="headerlink" title="methods proposed"></a>methods proposed</h2><h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><h2 id="discussions-1"><a href="#discussions-1" class="headerlink" title="discussions"></a>discussions</h2><h2 id="available-datasets-1"><a href="#available-datasets-1" class="headerlink" title="available datasets"></a>available datasets</h2><h2 id="some-inspirations-1"><a href="#some-inspirations-1" class="headerlink" title="some inspirations"></a>some inspirations</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/" data-id="ck9igurs40000j6fy8m2ndkd6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SLT/" rel="tag">SLT</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004261703-ARXIV2020-SLT2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/26/202004261703-ARXIV2020-SLT2/" class="article-date">
  <time datetime="2020-04-26T09:04:05.000Z" itemprop="datePublished">2020-04-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/26/202004261703-ARXIV2020-SLT2/">202004261703-ARXIV2020-SLT2</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="SIGN-LANGUAGE-TRANSLATION-WITH-TRANSFORMERS"><a href="#SIGN-LANGUAGE-TRANSLATION-WITH-TRANSFORMERS" class="headerlink" title="SIGN LANGUAGE TRANSLATION WITH TRANSFORMERS"></a>SIGN LANGUAGE TRANSLATION WITH TRANSFORMERS</h1><p><em>note at:</em> 202004232043</p>
<p><em>labels:</em> SLT</p>
<p><strong>conf:</strong> ??</p>
<p><strong>author:</strong> Kayo Yin,École Polytechnique</p>
<p><strong>codes available at:</strong> <a href="https://github.com/kayoyin/transformer-slt" target="_blank" rel="noopener">code link</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/26/202004261703-ARXIV2020-SLT2/framework.png" alt="framework overview"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><p>这篇文章提出，机器翻译在各种自然语言之间发展的不错了，也得到了很多的关注，但是在手语翻译方面目前的研究还很少。虽然之前有SLR，CSLR的工作，但是手语到对应的自然语言之间并不是简单的逐词识别问题，而是各有自己的语言规则和语法。CSLR只解决了手语视频到每个词的直接意思的理解，即sign2gloss这个过程，但是对于sign2text，CLSR是不能胜任的。</p>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><p>本文作者关注的不是从sign video到text的任务，而是gloss2text这个方面，也就是说本文其实只是把其他NMT的方法直接搬到了手语的gloss上，进而做出了gloss2text这一过程。至于sign2text任务，本文没有提出新的方法，仍然利用了2018年CVPR那篇文章中的思路，直接使用一个SOTA的CSLR模型来获得sign2gloss，再把得到的gloss送进本文的transformer模型中得到最终的text。</p>
<p>总体来说创新性不大，算是复习一遍BERT模型。</p>
<blockquote>
<p>BERT模型简介</p>
</blockquote>
<ul>
<li><p>encoder</p>
<ul>
<li>encoder和decoder在transformer based模型中都是stack了好几层，每层内部都一样，多层的结构是为了更好的理解输入信息。</li>
<li>每一个单层encoder中对于输入的embedding，先经过一个multi-head self attention。这里self attention的作用是为了获得输入信息的时序依赖关系，从而弥补网络结构不像RNN based和1d CNN based模型那种天然的时序信息。</li>
<li>过了self attention之后跟residual连接的数据add后再经过一个layer norm</li>
<li>然后送进linear layer based feed forward，同样这里也有residual link，然后也经过layer norm</li>
<li>接下来就是下一个encoder stack，后面跟上面一样</li>
</ul>
</li>
<li><p>multi-head self attention</p>
<ul>
<li>这里的注意力机制同样是三个输入Q(n x k),K(n x k),V(n x v),他们都是通过简单的FC网络获得的feature</li>
<li><img src="/2020/04/26/202004261703-ARXIV2020-SLT2/self-attention.jpeg" alt="multi-head self attention"></li>
</ul>
</li>
<li><p>decoder</p>
<ul>
<li>decoder跟encoder基本上是对称的结构，但是其第一个stack的attention模块是mask attention，如在解码第t个step的数据时，t后面step的sentence输入信息都需要被mask，第一个stack的输入是上一个step得到的输出。</li>
<li>后面所有的attention模块输入都是从encoder输出的feature+上一层的输出</li>
</ul>
</li>
<li><p>position encoding</p>
<ul>
<li>无论encoder还是decoder，其输入embedding都不是原始的word embedding，而是经过position embedding操作后的</li>
<li>这里position encoding操作仍然是因为self attention对于输入没有时间前后的概念，所以这里通过人为的编码，使得输入单词embedding的相对位置在编码中得到体现。</li>
<li><img src="/2020/04/26/202004261703-ARXIV2020-SLT2/position-encoding.png" alt="position encoding"></li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/26/202004261703-ARXIV2020-SLT2/" data-id="ck9h1gash0000bjfy51ifcuga" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SLR/" rel="tag">SLR</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004232043-CVPR2020-SLT" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/23/202004232043-CVPR2020-SLT/" class="article-date">
  <time datetime="2020-04-23T12:43:37.000Z" itemprop="datePublished">2020-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/23/202004232043-CVPR2020-SLT/">202004232043-CVPR2020-SLT</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Sign-Language-Transformers-Joint-End-to-end-Sign-Language-Recognition-and-Translation"><a href="#Sign-Language-Transformers-Joint-End-to-end-Sign-Language-Recognition-and-Translation" class="headerlink" title="Sign Language Transformers:Joint End-to-end Sign Language Recognition and Translation"></a>Sign Language Transformers:Joint End-to-end Sign Language Recognition and Translation</h1><p><em>note at:</em> 202004232043</p>
<p><em>labels:</em> SLT</p>
<p><strong>conf:</strong> CVPR 2020</p>
<p><strong>author:</strong> Necati Cihan Camgoz, Oscar Koller</p>
<p><strong>codes available at:</strong> <a href="https://github.com/neccam/slt" target="_blank" rel="noopener">code link</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/23/202004232043-CVPR2020-SLT/framework.png" alt="framework overview"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><blockquote>
<p>motivation</p>
</blockquote>
<p>本文的主要目的是希望通过end2end的方式，直接进行从sign language到自然语言text的手语翻译，同时利用中间态gloss标注作为中间监督，从而使得不会出现sign2gloss2text这种在gloss这里的瓶颈。</p>
<blockquote>
<p>problems</p>
</blockquote>
<ul>
<li>之前的大部分工作都是做CSLR任务，默认sign video跟text之间是一一对应的，没有复杂的语序等问题</li>
<li>之前的SLT工作利用了gloss中间态监督信息，但是其分步骤进行翻译的结构使得在得到gloss后形成了一个瓶颈，内在的失去了一些视频中的信息</li>
<li>手语本身也存在着难以理解的内在复杂性。手语是个集多种信息于一体的语言，在视频中不止有手，包括动作，头，身体，表情等等都是信息的携带点。此外，手语也是一种语言，跟spoken language一样，也是有自己的语法和词汇的，所以默认两者之间存在着简单的直接映射是不合理的。</li>
</ul>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><blockquote>
<p>手语处理流程</p>
</blockquote>
<ul>
<li>视觉信息理解<ul>
<li>这里，是个很重要的环节。在sign2text的模型中，缺少sign2gloss过程，这就相当于直接把视频翻译成自然语言，跨过了对视频的直接理解监督，这种模型的表现普遍不如利用了gloss的。而利用gloss则是相当于在中间加入了中间层监督，这样就可以更好的控制模型对输入视频的正确理解过程。</li>
</ul>
</li>
<li>视觉信息到自然语言的映射</li>
</ul>
<blockquote>
<p>method details</p>
</blockquote>
<p><img src="/2020/04/23/202004232043-CVPR2020-SLT/method.png" alt="method framework for single layer"></p>
<p>整个网络分为三个部分：</p>
<ul>
<li>SLRT(Sign Language Recognition Transformers)<ul>
<li>整个网络框架是利用了BERT的结构，整个思路也基本上是一样的，只是输入信息不同。（BERT模型需要另做学习，这里留一个链接坑位吧<a href="？？">BERT 学习笔记</a>）<a href="https://github.com/codertimo/BERT-pytorch" target="_blank" rel="noopener">pytorch implement of BERT</a></li>
<li>input：视频编码——利用CNN提取视频帧的feature。（这里是重要的改进点！！！）</li>
<li>position embedding：同样是bert模型中的东西，因为基于transformer的模型的并行性，不像RNN那样有着天然的时序信息，这里通过position embedding来对时序进行建模。</li>
<li>后面又通过self attention来进一步加强了输入视频帧各帧feature之间的语义关系（这里可能是已经能够建模前后帧之间的关系了，如果想在这里加强上下文信息关系，需要好好思考如何进行改进，这个地方更像是纯粹的NLP问题，如果有好的方式，在其他NLP领域应该也会很有用）</li>
<li>后面经过了feed forward之后又经过了FC（linear）然后softmax算概率，最后送到CTC模块来从frame level到gloss level</li>
</ul>
</li>
<li>CTC<ul>
<li>这个模块具体内容需要更进一步学习，包括看相关代码，在NLP任务中很重要</li>
<li>CTC算法的详细讲解博客：<a href="https://xiaodu.io/ctc-explained/" target="_blank" rel="noopener">CTC 算法解析 part1</a>，<a href="https://xiaodu.io/ctc-explained-part2/" target="_blank" rel="noopener">CTC 算法解析 part2</a>; 百度CTC loss工具包：<a href="https://github.com/baidu-research/warp-ctc" target="_blank" rel="noopener">warp-ctc</a>；<a href="https://github.com/SeanNaren/warp-ctc" target="_blank" rel="noopener">another pytorch implement</a></li>
</ul>
</li>
<li>SLTT(Sign Language Translation Transformers)<ul>
<li>这个相当于是decoder，跟bert模型完全一样的</li>
</ul>
</li>
</ul>
<h2 id="experiments-and-results"><a href="#experiments-and-results" class="headerlink" title="experiments and results"></a>experiments and results</h2><blockquote>
<p>results</p>
</blockquote>
<p><img src="/2020/04/23/202004232043-CVPR2020-SLT/result1.png" alt="result1"></p>
<p><img src="/2020/04/23/202004232043-CVPR2020-SLT/result2.png" alt="result2"></p>
<p><img src="/2020/04/23/202004232043-CVPR2020-SLT/result3.png" alt="result3"></p>
<h2 id="some-comments"><a href="#some-comments" class="headerlink" title="some comments"></a>some comments</h2><p>本文总体来说算是第一次正经的好好解决了SLT的问题，虽然两年前的nslt算是开山之作，但是当时的方法很直接，也很粗暴，相当于是CSLR+NMT的简单使用，并没有做的更细致。而这一篇工作就算是SLT应该有的处理方式了，通过CNN提取视频特征，通过跟视频时序逐次对应的gloss来作为视频理解的中间监督，通过CTC来解决视频维度远大于gloss词维度的问题，然后利用更新的NMT中基于BERT的网络框架来处理翻译任务。</p>
<p>算是一个很不错的baseline，直观简单，但是确实应该这么做。后面应该还有很大改进的空间，需要在视频理解这里花一些功夫。还有就是视频特征是否可以进行更稀疏的处理，减少不必要的冗余信息。</p>
<h2 id="references-must-be-checked"><a href="#references-must-be-checked" class="headerlink" title="references must be checked!"></a>references must be checked!</h2><ul>
<li>StephanieStoll,NecatiCihanCamgoz,SimonHadfield,and Richard Bowden. <em>Sign Language Production using Neural Machine Translation and Generative Adversarial Networks.</em> In Proceedings of the British Machine Vision Conference (BMVC), 2018.</li>
<li>StephanieStoll,NecatiCihanCamgoz,SimonHadfield,and Richard Bowden. <em>Text2Sign: Towards Sign Language Pro- duction using Neural Machine Translation and Generative Adversarial Networks.</em> International Journal of Computer Vision (IJCV), 2020.</li>
<li>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. <em>Attention is All You Need.</em> In Proceedings of the Advances in Neural Information Processing Systems (NIPS), 2017.</li>
</ul>
<h2 id="inspirations"><a href="#inspirations" class="headerlink" title="inspirations"></a>inspirations</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/23/202004232043-CVPR2020-SLT/" data-id="ck9cxe8b300002kfyczbv64df" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CVPR2020/" rel="tag">CVPR2020</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SLR/" rel="tag">SLR</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004221543-CVPR2020-MusicGesture" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/22/202004221543-CVPR2020-MusicGesture/" class="article-date">
  <time datetime="2020-04-22T07:42:46.000Z" itemprop="datePublished">2020-04-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/22/202004221543-CVPR2020-MusicGesture/">202004221543-CVPR2020-MusicGesture</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="A2J-Anchor-to-Joint-Regression-Network-for-3D-Articulated-Pose-Estimation-from-a-Single-Depth-Image"><a href="#A2J-Anchor-to-Joint-Regression-Network-for-3D-Articulated-Pose-Estimation-from-a-Single-Depth-Image" class="headerlink" title="A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image"></a>A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</h1><p><em>note at:</em> 202004221543</p>
<p><em>labels:</em> others</p>
<p><strong>conf:</strong> CVPR 2020</p>
<p><strong>author:</strong> Chuang Gan (MIT, MIT-IBM Watson AI Lab)</p>
<p><strong>codes available at:</strong> <a href="http://music-gesture.csail.mit.edu" target="_blank" rel="noopener">project page</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/22/202004221543-CVPR2020-MusicGesture/framework.png" alt="framework"></p>
<h2 id="problems-to-be-solved-task-definition"><a href="#problems-to-be-solved-task-definition" class="headerlink" title="problems to be solved (task definition)"></a>problems to be solved (task definition)</h2><p>问题定义：<br>把不同乐器（不同种，同种不同实例）的音频从视频中各自分离出来</p>
<p>现有方法的问题：</p>
<ul>
<li>虽然现在已经有了一些这个方向的工作，也有工作开始使用visual信息来帮助分离音频。之前的工作都发现了语义信息（semantic appearances）对模型会有帮助，但是这些工作没有考虑视频中的motion cues，这样就限制了这些方法在更复杂的场景下的表现</li>
<li>最近有人利用了temporal motion信息，但是他们基于运动轨迹和光流这样的特征（trajectory and optical flow like features），仍然是局限在对human-object interactions的建模上，没有关注到更细节的地方。</li>
</ul>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><blockquote>
<p>motivation<br>本文中，作者希望利用基于人体关键点的motion cues，同时利用visual feature和sound feature来进行音频分离（sound source separation）</p>
</blockquote>
<blockquote>
<p>method details<br>作者主要在于第一次在这个细分领域使用了关键点信息来表示motion，并且整个work的模型搭建的比较完整，make sense。</p>
</blockquote>
<p>主要贡献在三个地方：</p>
<ul>
<li><p>Video Analysis Network</p>
<ul>
<li><p>这里主要是利用了之前用在其他领域的工作来解决新的问题</p>
</li>
<li><p>利用ResNet50来提取视频的首帧特征，用于做visual context feature</p>
</li>
<li><p>用了ST-GCN（Spatial tempo- ral graph convolutional networks for skeleton-based action recognition）来对提取的人体关键点在时空上建立图模型</p>
</li>
<li><p>利用了alphaPose来提取body关键点，用了一个预训练过的hand detection网络来检测双手，然后利用openpose来提取双手关键点</p>
</li>
<li><p>网络细节：</p>
</li>
<li><p>这里的GCN有11层，并带有residual link。输入是坐标点+置信度(x,y,c),并且在输入网络之前经过了BN层来使得input scale保持相同（这里还需要看看code理解细节！！！）</p>
</li>
<li><p>hand detection网络没有给引用和细节，估计是从git上找了一个</p>
</li>
<li><p>在关键点经过GCN处理后，得到的motion feature又跟之前获得的visual context feature进行了拼接，从而加入了图像信息（这个点有点意思的在于，后面的ablation实验的数据加上这个还不如不加，然而作者并没有给出合理的解释~~）</p>
</li>
</ul>
</li>
<li><p>Audio-Visual Separation Network</p>
<ul>
<li><p>这里作者利用了一个U-Net类型的网络，encoder-decoder结构</p>
</li>
<li><p>encoder输入时2d 音频的频谱</p>
</li>
<li><p>音频信号经encoder得到音频特征后，跟前面的visual+motion feature进行了融合（基于self attention，见下面Audio-visual fusion）</p>
</li>
<li><p>然后fusion feature经过对称的decoder得到最终的audio mask</p>
</li>
<li><p>网络细节：</p>
</li>
<li><p>encoder和decoder对称，都是4层空洞卷积（3 x 3 spatial filters with stride 2, dilation 1 and followed by a BatchNorm layer and a Leaky ReLU）</p>
</li>
</ul>
</li>
<li><p>Audio-visual fusion</p>
<ul>
<li>sound feature和visual feature先经过矩阵乘后softmax来得到attention matrix</li>
<li>然后利用该matrix跟vision feature矩阵乘，再跟sound feature进行相加</li>
<li>得到的结果再经过基于MLP的残差网络得到最终的fusion feature</li>
<li><img src="/2020/04/22/202004221543-CVPR2020-MusicGesture/feature-fusion.png" alt="self attention based Audio-visual fusion"></li>
<li>The MLP is implemented with two fully-connected layers with a ReLU activation function.</li>
</ul>
</li>
</ul>
<p>至于loss什么的细节，文章本身讲的不多，也不是这篇文章的主要关注点，在此略过。</p>
<h2 id="experiments-and-results"><a href="#experiments-and-results" class="headerlink" title="experiments and results"></a>experiments and results</h2><p><img src="/2020/04/22/202004221543-CVPR2020-MusicGesture/result1.png" alt="result1"><br><img src="/2020/04/22/202004221543-CVPR2020-MusicGesture/result2.png" alt="result2"><br>这里的ablation实验也很简单，但是确实有上面提到的问题。</p>
<h2 id="comments"><a href="#comments" class="headerlink" title="comments"></a>comments</h2><p>本文不是自己的主业，看了主要是增多一点视频理解的储备知识。<br>从实验结果上来看，基于keypoints和GCN的方式确实是对这种视频的理解有挺大帮助的。</p>
<h2 id="inspiration"><a href="#inspiration" class="headerlink" title="inspiration"></a>inspiration</h2><p>本文的思路在对手语翻译这个方向上应该来说有很大的借鉴意义，虽然说文中的一些方法已经被别人用过了，也不算什么新方法。不过其中的feature fusion的方式值得学习一波，手语的不同部位可以看做不同的信息源，融合不同部位的信息应该来说是有用的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/22/202004221543-CVPR2020-MusicGesture/" data-id="ck9b7lw8k00004vfy8ijeeszj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CVPR2020/" rel="tag">CVPR2020</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/others/" rel="tag">others</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004211039-BMVC-HMR" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/21/202004211039-BMVC-HMR/" class="article-date">
  <time datetime="2020-04-21T02:39:55.000Z" itemprop="datePublished">2020-04-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/21/202004211039-BMVC-HMR/">202004211039-BMVC-HMR</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Single-Image-3D-Hand-Reconstruction-with-Mesh-Convolutions"><a href="#Single-Image-3D-Hand-Reconstruction-with-Mesh-Convolutions" class="headerlink" title="Single Image 3D Hand Reconstruction with Mesh Convolutions"></a>Single Image 3D Hand Reconstruction with Mesh Convolutions</h1><p><em>note at:</em> 202004211039</p>
<p><em>labels:</em> hand pose mesh</p>
<p><strong>conf:</strong> BMVC 2019(CCF C)</p>
<p><strong>author:</strong> Dominik Kulon</p>
<p><strong>codes available at:</strong> <a href="https://github.com/dkulon/hand-reconstruction" target="_blank" rel="noopener">code link</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/21/202004211039-BMVC-HMR/framework.png" alt="framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><p>首先，这篇文章关注的问题是hand mesh recovery，并且是单人的，也就是说，hand没有scale variation</p>
<p>在此之前，MANO模型解决了这个问题，但是作者提出，SMPL based模型的参数太过，计算比较慢。并且MANO的顶点数太少，对细节表达能力不足</p>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><p>这篇文章描述比之顶会还是差了很远，方法相对来说很简单，也没有很多insight，算是做了一个数据集，一个baseline。（代码公布了，感觉做recovery的话还是可以借鉴一下的）</p>
<blockquote>
<p>main contribution</p>
</blockquote>
<p>作者主要做了三件事</p>
<ul>
<li>做了一个数据集<ul>
<li>具体制作过程在原文第5章</li>
<li>每个mesh有7907个定点，是MANO的十倍还多</li>
<li>包含40360个训练实例，3000个验证实例，3000个测试实例。</li>
<li>标注包括，mesh，3d keypoint（投影到图像坐标系）</li>
</ul>
</li>
</ul>
<ul>
<li>做了一个在hand mesh上的autoencoder，输入mesh，得到mesh<ul>
<li>利用了卷积（看文章的意思，貌似是利用了chebvGCN一样的图卷积）</li>
<li>encoder：输入mesh定点，经过4个卷积层（conv+down sampling+lReLU）</li>
<li>经过一个FC层（64）</li>
<li>decoder：跟encoder对称（这个东西是真正想要的，为了后面从RGB得到mesh而训练）</li>
</ul>
</li>
</ul>
<ul>
<li>single RGB image到相应的hand mesh<ul>
<li>image encoder：利用了DenseNet-121提取图像特征，在imageNet上做了预训练，输出两部分：1)用于mesh decoder的mesh embeding，2)用于回归相机参数的camera embeding</li>
<li>mesh decoder：就是前面autoencoder中的decoder，这里称之为Graph Morphable Model (GMM)——输入为64维隐层特征向量，该向量是image encoder的输出</li>
<li>camera decoder：用于回归得到一个将mesh投影到跟图像的视角一致的相机参数，输入为image encoder得到的camera embedding，经过几层FC层得到相机参数。</li>
<li>loss：mesh上的L1，投影得到的keypoint joints对应的L1，RGB经过image encoder得到的embedding（64维向量）跟mesh autoencoder中encoder对GT mesh进行编码得到的embedding之间的L2。(三个loss的加权系数分别为1，0.01，5e-5，并使用了L2正则，正则系数1e-5)<img src="/2020/04/21/202004211039-BMVC-HMR/loss.png" alt="objective func"></li>
</ul>
</li>
</ul>
<h2 id="discussions"><a href="#discussions" class="headerlink" title="discussions"></a>discussions</h2><p>这篇文章思路很简单，主要功夫在做数据集上，但是这个数据集又有些局限。并且文章叙述确实是不好，很多地方不太清楚，同时实验部分不清晰，实验结果甚至都看不到，论证的点很多没有相应的实验佐证。</p>
<p>但无论如何，开源了代码，作为一个baseline的参考还是有价值的。</p>
<h2 id="available-datasets"><a href="#available-datasets" class="headerlink" title="available datasets"></a>available datasets</h2><p>需要通过发送邮件来跟作者要</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/21/202004211039-BMVC-HMR/" data-id="ck99ozik50000b9fy4lgj58hu" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hand-pose-mesh/" rel="tag">hand pose mesh</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/others/" rel="tag">others</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004151649-ICCV2019-A2J" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/20/202004151649-ICCV2019-A2J/" class="article-date">
  <time datetime="2020-04-20T09:59:34.000Z" itemprop="datePublished">2020-04-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/20/202004151649-ICCV2019-A2J/">202004151649-ICCV2019-A2J</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="A2J-Anchor-to-Joint-Regression-Network-for-3D-Articulated-Pose-Estimation-from-a-Single-Depth-Image"><a href="#A2J-Anchor-to-Joint-Regression-Network-for-3D-Articulated-Pose-Estimation-from-a-Single-Depth-Image" class="headerlink" title="A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image"></a>A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</h1><p><em>note at:</em> 202004151649</p>
<p><em>labels:</em> 3d hand pose esti depth</p>
<p><strong>conf:</strong> ICCV 2019</p>
<p><strong>author:</strong> YUAN Junsong</p>
<p><strong>codes available at:</strong> <a href="https://github.com/zhangboshen/A2J" target="_blank" rel="noopener">code link</a></p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/20/202004151649-ICCV2019-A2J/framework1.png" alt="main idea"></p>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/framework2.png" alt="framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><ul>
<li>现有基于encoder-decoder FCN方式的工作基本上都利用gaussian heatmap这种non-adaptive的训练方式，其计算代价大，并且难以end2end</li>
<li>3D CNN的方式需要训练的参数多，并且体素化操作比较麻烦且开销大</li>
<li>基于point-set的方式则需要花费很多时间在数据预处理上</li>
</ul>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><blockquote>
<h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3></blockquote>
<p>借鉴anchor based方式，利用anchor来获得keypoints坐标。key idea是：通过多个预测anchor的偏移，然后加权求和从而获得3d pose预测，利用整体学习来获得更好的泛化性。(The key idea of A2J is to predict 3D joint position by aggregating the estimation results of multiple anchor points, in spirit of en- semble learning to enhance generalization ability.)</p>
<h3 id="method-details"><a href="#method-details" class="headerlink" title="method details"></a>method details</h3><ul>
<li>ResNet50 做backbone来提取特征，前三层提取common feature，最后一层提取semantic feature</li>
<li>in-plain offset estimation branch来获得anchor的offset</li>
<li>depth estimation branch来获得关键点的深度</li>
<li>anchor proposal branch来获得加权权重</li>
</ul>
<blockquote>
<p>in-plain offset estimation branch</p>
</blockquote>
<ul>
<li>input: 从backbone提取的semantic feature(trunk), </li>
<li>经过4个conv2d + BN + ReLU，</li>
<li>最后经过一个output conv（16 x K x 2 通道）得到offsets，</li>
<li>这里的16代表，每个feature point代表了16个anchor points（因为input是image经过16倍下采样的，而每个anchor是在image上每隔4个point取一个做anchor set）</li>
</ul>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/in-plain.png" alt="in-plain and depth estimation"></p>
<blockquote>
<p>depth estimation branch</p>
</blockquote>
<ul>
<li>前边跟in-plain branch一样</li>
<li>最后经过一个output conv（16 x K x 1 通道）得到depth</li>
</ul>
<blockquote>
<p>anchor proposal branch</p>
</blockquote>
<ul>
<li>input: 从backbone提取的common feature(trunk),</li>
<li>同样经过4个conv2d + BN + ReLU，</li>
<li>最后经过一个output conv（16 x K x 1 通道）得到加权值</li>
</ul>
<blockquote>
<h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3></blockquote>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/loss1.png" alt="loss1"><br><img src="/2020/04/20/202004151649-ICCV2019-A2J/loss2.png" alt="loss2"></p>
<p>loss = 3loss1 + loss2</p>
<h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><blockquote>
<p>comparison with SOTA</p>
</blockquote>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/comparision.png" alt="comparison with SOTA"></p>
<blockquote>
<p>ablation</p>
</blockquote>
<p><img src="/2020/04/20/202004151649-ICCV2019-A2J/ablation.png" alt="ablation"></p>
<h2 id="discussions"><a href="#discussions" class="headerlink" title="discussions"></a>discussions</h2><h2 id="available-datasets"><a href="#available-datasets" class="headerlink" title="available datasets"></a>available datasets</h2><p>NYU: </p>
<p><a href="https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm#download" target="_blank" rel="noopener">website</a> </p>
<p><a href="http://horatio.cs.nyu.edu/mit/tompson/nyu_hand_dataset_v2.zip" target="_blank" rel="noopener">data</a></p>
<h2 id="some-inspirations"><a href="#some-inspirations" class="headerlink" title="some inspirations"></a>some inspirations</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/20/202004151649-ICCV2019-A2J/" data-id="ck98baczg0000kifyecrkhiuk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ICCV2019/" rel="tag">ICCV2019</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hand-pose-depth/" rel="tag">hand pose depth</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-202004060009-ICCV2019-HAMR" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/18/202004060009-ICCV2019-HAMR/" class="article-date">
  <time datetime="2020-04-18T12:18:00.000Z" itemprop="datePublished">2020-04-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/18/202004060009-ICCV2019-HAMR/">202004060009-ICCV2019-HAMR</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="End-to-end-Hand-Mesh-Recovery-from-a-Monocular-RGB-Image"><a href="#End-to-end-Hand-Mesh-Recovery-from-a-Monocular-RGB-Image" class="headerlink" title="End-to-end Hand Mesh Recovery from a Monocular RGB Image"></a>End-to-end Hand Mesh Recovery from a Monocular RGB Image</h1><p><em>note at:</em> 202004060009</p>
<p><strong>conf:</strong> ICCV 2019</p>
<p><strong>author:</strong> Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, Wen Zheng</p>
<p><strong>codes available at:</strong> <a href="https://github.com/Wavelet303/HAMR" target="_blank" rel="noopener">code link</a>(only inference code available)</p>
<h2 id="framework"><a href="#framework" class="headerlink" title="framework"></a>framework</h2><p><img src="/2020/04/18/202004060009-ICCV2019-HAMR/HAMR-framework.png" alt="HAMR-framework"></p>
<h2 id="problems-to-be-solved"><a href="#problems-to-be-solved" class="headerlink" title="problems to be solved"></a>problems to be solved</h2><ul>
<li>single RGB 图像缺少深度信息，对于hand这种自遮挡较严重，表达复杂多样的任务，具有较大难度。</li>
<li>数据标注难度大，目前有几个人工合成的数据，但是合成数据跟真实的数据之间是存在gap的</li>
</ul>
<h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>利用参数化hand model，从single RGB通过端到端方式重建出对应的hand mesh，利用这一更高维的表达可以进一步获得低维的2d，3d pose。</p>
<h2 id="methods-proposed"><a href="#methods-proposed" class="headerlink" title="methods proposed"></a>methods proposed</h2><p>总体来讲，本文的思路就是利用2d pose estimation的工作作为head，提取出RGB图像的特征并得到2d joints的heatmap，然后利用heatmap和图像特征，进一步通过网络迭代的学习到相机参数和mesh参数，进而利用参数获得mesh。</p>
<p>具体来说：</p>
<p>对于2d pose estimation部分：<br>作者利用了经典的hourglass网络，但做出了一些调整：</p>
<ul>
<li>使用3*3卷积替代了原网络中的residual module</li>
<li>average pooling替换成了原网络的max pooling</li>
<li>在每个3*3卷积后增加了BN层</li>
<li>不止获得21个joints heatmap，增加了一些冗余来增加特征</li>
</ul>
<p>对于参数回归（Iterative Regression Module）：<br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/regression.png" alt="Iterative Regression Module"></p>
<ul>
<li>输入包括joints heatmap，冗余feature maps，hourglass的中间层feature maps</li>
<li>网络结构是简单的全卷积+三层全连接层</li>
<li>最后输出两个参数（相机参数，mesh参数）</li>
<li>整个过程是迭代进行的</li>
</ul>
<p>其中相机参数（s, tx, ty）由三个数组成。GT——在知道2d，3d坐标后，可以反向求解得到该参数。<br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/coord2K.png" alt="coord2K"><br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/3d-2d.png" alt="3d-2d"></p>
<p>mesh参数（MANO参数）：</p>
<ul>
<li>β： shape参数，10维，控制hand的具体shape信息，如胖瘦，手指粗细等，是一种PCA的参数</li>
<li>$\theta$：pose参数，K*3维，控制手的姿态，可以理解为K个关键点用Rodrigues向量表示下的3D旋转</li>
<li>根据这两个参数，MANO模型可以生成相应的hand mesh</li>
</ul>
<h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3><ul>
<li>heatmap loss：pixel wise distance（应该是逐像素L1）</li>
<li>2d loss：L2 loss</li>
<li>3d loss：L2 loss</li>
<li>geometry loss：每只指头4关节共面&amp;&amp;每只手指的三个骨头相对旋转角度方向一致，即向量表示的向量积&gt;0</li>
<li>camera loss：相机内参loss，L2 loss</li>
<li>seg loss：mesh的投影跟hand的GT silhouette之间的 L1 loss</li>
</ul>
<h2 id="results"><a href="#results" class="headerlink" title="results"></a>results</h2><p><img src="/2020/04/18/202004060009-ICCV2019-HAMR/result1.png" alt="coord2K"><br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/result2.png" alt="3d-2d"><br><img src="/2020/04/18/202004060009-ICCV2019-HAMR/result3.png" alt="coord2K"></p>
<h2 id="discussions"><a href="#discussions" class="headerlink" title="discussions"></a>discussions</h2><h2 id="some-inspirations"><a href="#some-inspirations" class="headerlink" title="some inspirations"></a>some inspirations</h2><p>本文算是很直接的利用MANO模型的例子，没有很复杂的网络结构，思路也非常直接，总体来说亮点不算很明显，主要在于做的早，很快的把人体的工作迁移到了hand上。</p>
<p>值得注意的是，本文同样是尽可能的利用了各种能用的监督信息。这一点在hand任务中应该确实是挺重要的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhang-mohole.github.io/2020/04/18/202004060009-ICCV2019-HAMR/" data-id="ck95mfta40000onfy46z935ux" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ICCV2019/" rel="tag">ICCV2019</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hand-pose-mesh/" rel="tag">hand pose mesh</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR2020/" rel="tag">CVPR2020</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ECCV2018/" rel="tag">ECCV2018</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICCV2019/" rel="tag">ICCV2019</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SLR/" rel="tag">SLR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SLT/" rel="tag">SLT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hand-pose-RGB/" rel="tag">hand pose RGB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hand-pose-depth/" rel="tag">hand pose depth</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hand-pose-mesh/" rel="tag">hand pose mesh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hand-object/" rel="tag">hand+object</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/others/" rel="tag">others</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CVPR2020/" style="font-size: 20px;">CVPR2020</a> <a href="/tags/ECCV2018/" style="font-size: 10px;">ECCV2018</a> <a href="/tags/ICCV2019/" style="font-size: 15px;">ICCV2019</a> <a href="/tags/SLR/" style="font-size: 15px;">SLR</a> <a href="/tags/SLT/" style="font-size: 15px;">SLT</a> <a href="/tags/hand-pose-RGB/" style="font-size: 10px;">hand pose RGB</a> <a href="/tags/hand-pose-depth/" style="font-size: 10px;">hand pose depth</a> <a href="/tags/hand-pose-mesh/" style="font-size: 15px;">hand pose mesh</a> <a href="/tags/hand-object/" style="font-size: 10px;">hand+object</a> <a href="/tags/others/" style="font-size: 15px;">others</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/05/07/202005072249-ARXIV2020-BMVC-SLP-transformer/">202005072249-ARXIV2020_BMVC-SLP_transformer</a>
          </li>
        
          <li>
            <a href="/2020/05/04/202005042101-CVPR2020-PhotometricH-O/">202005042101-CVPR2020-PhotometricH+O</a>
          </li>
        
          <li>
            <a href="/2020/04/29/202004080031-ECCV2018-2-5D-heatmap/">202004080031-ECCV2018-2.5D_heatmap</a>
          </li>
        
          <li>
            <a href="/2020/04/27/202004271600-BMVC2018IJCV2020-SLP/">202004271600-WACV2018IJCV2020-SLP</a>
          </li>
        
          <li>
            <a href="/2020/04/26/202004261703-ARXIV2020-SLT2/">202004261703-ARXIV2020-SLT2</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>